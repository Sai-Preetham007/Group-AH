{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medical Model Training Notebook\n",
        "\n",
        "This notebook demonstrates how to train medical language models using the Kaggle disease dataset.\n",
        "\n",
        "## Overview\n",
        "- Load processed Q&A data\n",
        "- Create PyTorch datasets\n",
        "- Initialize medical language model\n",
        "- Train the model\n",
        "- Evaluate performance\n",
        "- Save trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, \n",
        "    TrainingArguments, Trainer,\n",
        "    AutoModelForCausalLM, AutoModelForSequenceClassification\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed Q&A data\n",
        "data_path = Path(\"../data/processed/train_qa_pairs.json\")\n",
        "test_data_path = Path(\"../data/processed/test_qa_pairs.json\")\n",
        "\n",
        "with open(data_path, \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(test_data_path, \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f\"Training data: {len(train_data)} samples\")\n",
        "print(f\"Test data: {len(test_data)} samples\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample training data:\")\n",
        "print(f\"Question: {train_data[0]['question']}\")\n",
        "print(f\"Answer: {train_data[0]['answer'][:100]}...\")\n",
        "print(f\"Disease: {train_data[0]['disease']}\")\n",
        "print(f\"Symptoms: {train_data[0]['symptoms']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create PyTorch Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PyTorch Dataset for medical Q&A\n",
        "class MedicalQADataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Add special tokens\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Create input text\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        \n",
        "        # Format for causal language modeling\n",
        "        text = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "        \n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': encoding['input_ids'].squeeze()  # For causal LM, labels are same as input_ids\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MedicalQADataset(train_data, tokenizer)\n",
        "test_dataset = MedicalQADataset(test_data, tokenizer)\n",
        "\n",
        "print(f\"Created datasets:\")\n",
        "print(f\"Train: {len(train_dataset)} samples\")\n",
        "print(f\"Test: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Model and Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../data/models/medical_model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"../logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate and Test the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Test the model with sample questions\n",
        "def generate_response(question, max_length=200):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "# Test with sample questions\n",
        "test_questions = [\n",
        "    \"What disease causes fever and cough?\",\n",
        "    \"I have itching and skin rash, what could it be?\",\n",
        "    \"What condition is associated with chills and fever?\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting model with sample questions:\")\n",
        "for question in test_questions:\n",
        "    response = generate_response(question)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
