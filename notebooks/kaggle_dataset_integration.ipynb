{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle Disease Dataset Integration\n",
        "\n",
        "This notebook demonstrates how to download, process, and integrate the Kaggle disease-symptom dataset into the Medical RAG system.\n",
        "\n",
        "## Overview\n",
        "- Load Kaggle Disease Dataset\n",
        "- Process symptoms and diseases\n",
        "- Create Q&A pairs for training\n",
        "- Generate knowledge base\n",
        "- Test dataset integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Process Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Kaggle dataset\n",
        "dataset_path = Path(\"../data/raw/Disease Dataset/dataset.csv\")\n",
        "descriptions_path = Path(\"../data/raw/Disease Dataset/symptom_Description.csv\")\n",
        "precautions_path = Path(\"../data/raw/Disease Dataset/symptom_precaution.csv\")\n",
        "\n",
        "print(\"Loading Kaggle Disease Dataset...\")\n",
        "\n",
        "# Load main dataset\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(f\"Main dataset loaded: {len(df)} records\")\n",
        "\n",
        "# Load descriptions\n",
        "descriptions_df = pd.read_csv(descriptions_path)\n",
        "print(f\"Descriptions loaded: {len(descriptions_df)} diseases\")\n",
        "\n",
        "# Load precautions\n",
        "precautions_df = pd.read_csv(precautions_path)\n",
        "print(f\"Precautions loaded: {len(precautions_df)} diseases\")\n",
        "\n",
        "# Merge datasets\n",
        "df_merged = df.merge(descriptions_df, on='Disease', how='left')\n",
        "df_merged = df_merged.merge(precautions_df, on='Disease', how='left')\n",
        "\n",
        "print(f\"Final merged dataset: {len(df_merged)} records with {len(df_merged.columns)} columns\")\n",
        "df_merged.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Q&A Pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Q&A pairs for training\n",
        "qa_pairs = []\n",
        "\n",
        "for _, row in df_merged.iterrows():\n",
        "    # Extract symptoms\n",
        "    symptoms = []\n",
        "    for col in [f'Symptom_{i}' for i in range(1, 18)]:\n",
        "        if pd.notna(row[col]) and row[col].strip():\n",
        "            symptoms.append(row[col])\n",
        "    \n",
        "    if len(symptoms) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Create different types of questions\n",
        "    questions = [\n",
        "        f\"What disease causes {', '.join(symptoms[:3])}?\",\n",
        "        f\"I have {', '.join(symptoms[:2])}, what could be wrong?\",\n",
        "        f\"What condition is associated with {symptoms[0]} and {symptoms[1] if len(symptoms) > 1 else 'other symptoms'}?\",\n",
        "        f\"Help me identify the disease with symptoms: {', '.join(symptoms[:4])}\"\n",
        "    ]\n",
        "    \n",
        "    # Create comprehensive answer\n",
        "    answer_parts = [f\"The disease is {row['Disease']}.\"]\n",
        "    \n",
        "    if pd.notna(row.get('Description', '')):\n",
        "        answer_parts.append(f\"Description: {row['Description']}\")\n",
        "    \n",
        "    if pd.notna(row.get('Precaution_1', '')):\n",
        "        precautions = [row[f'Precaution_{i}'] for i in range(1, 5) if pd.notna(row.get(f'Precaution_{i}', ''))]\n",
        "        answer_parts.append(f\"Precautions: {', '.join(precautions)}\")\n",
        "    \n",
        "    answer = \" \".join(answer_parts)\n",
        "    \n",
        "    # Add Q&A pairs\n",
        "    for question in questions:\n",
        "        qa_pairs.append({\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'disease': row['Disease'],\n",
        "            'symptoms': symptoms,\n",
        "            'context': f\"Disease: {row['Disease']}, Symptoms: {', '.join(symptoms)}\"\n",
        "        })\n",
        "\n",
        "print(f\"Created {len(qa_pairs)} Q&A pairs\")\n",
        "print(f\"Sample Q&A pair:\")\n",
        "print(f\"Q: {qa_pairs[0]['question']}\")\n",
        "print(f\"A: {qa_pairs[0]['answer'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train-Test Split and Save Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "train_data, test_data = train_test_split(qa_pairs, test_size=0.2, random_state=42, stratify=[qa['disease'] for qa in qa_pairs])\n",
        "\n",
        "print(f\"Training data: {len(train_data)} samples\")\n",
        "print(f\"Test data: {len(test_data)} samples\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(\"../data/processed\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save processed data\n",
        "with open(output_dir / \"qa_pairs.json\", \"w\") as f:\n",
        "    json.dump(qa_pairs, f, indent=2)\n",
        "\n",
        "with open(output_dir / \"train_qa_pairs.json\", \"w\") as f:\n",
        "    json.dump(train_data, f, indent=2)\n",
        "\n",
        "with open(output_dir / \"test_qa_pairs.json\", \"w\") as f:\n",
        "    json.dump(test_data, f, indent=2)\n",
        "\n",
        "print(\"Processed data saved to data/processed/\")\n",
        "print(f\"- qa_pairs.json: {len(qa_pairs)} total pairs\")\n",
        "print(f\"- train_qa_pairs.json: {len(train_data)} training pairs\")\n",
        "print(f\"- test_qa_pairs.json: {len(test_data)} test pairs\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
